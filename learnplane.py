# Bismillahi-r-Rahmani-r-Rahim

# Test of whether we can learn a cone using SVMs

import random

import numpy as np
from matplotlib import pyplot as pl
from matplotlib.patches import Polygon
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import fbeta_score, f1_score, precision_score, recall_score
from SvmLightEstimator import SvmLightEstimator

import svmlight as svm

def get_cone_data():
    """
    Get points in the unit square and classifications assuming
    a cone generated by (1,3) and (1,1)
    """
    data = []
    for i in range(1000):
        x = np.random.uniform()
        y = np.random.uniform()
        in_cone = (y/x) >= 1./3 and (y/x) <= 1.
        #in_cone = (y/x) <= 0.7
        data += [(x,y,in_cone)]
    return data

def score(y_true, y_pred, labels=None, pos_label=1, average='weighted'):
    return fbeta_score(y_true, y_pred, 2, labels, pos_label, average)

def cross_validate(data, class_values):
    pars = {'cost': [5**x for x in range(4)],
            'cost_ratio': [x/10.0 for x in range(10, 20)] + [10.0/x for x in range(11, 20)]}
    search = GridSearchCV(SvmLightEstimator(), pars, score_func = score)
    search.fit(data, class_values)
    return search

def get_docs_and_class_values(data):
    docs = []
    for i in range(len(data)):
        docs.append(svm.Document(i, svm.SupportVector(
                    [(1, data[i][0]), (2, data[i][1])])))
    m = {True: 1, False: -1}
    class_values = [m[x[2]] for x in data]
    return docs, class_values

def plot(data, plane, name, limits=[(0.,1.),(0.,1.)]):
    pl.clf()
    pos = [x for x in data if x[2]]
    neg = [x for x in data if not x[2]]

    gradient = -plane[0]/plane[1]

    def plane_func(x):
        return gradient*x

    ax = pl.subplot(111)

    pl.plot([0,1], [0,gradient], linewidth=1)

    # make the shaded region
    up = plane[1] > 0
    verts = [(-2,-2*gradient),
             (-2 + plane[0], -2*gradient + plane[1]),
             (2 + plane[0], 2*gradient + plane[1]),
             (2, 2*gradient)]
    poly = Polygon(verts, facecolor='0.8', edgecolor='k')
    ax.add_patch(poly)
    #pl.show()

    pl.plot([x[0] for x in pos], [x[1] for x in pos], 'ro')
    pl.plot([x[0] for x in neg], [x[1] for x in neg], 'bx')

    ax.set_xlim(left=limits[0][0], right=limits[0][1])
    ax.set_ylim(bottom=limits[1][0], top=limits[1][1])
    pl.savefig(name)

if __name__ == "__main__":
    data = get_cone_data()
    docs, class_values = get_docs_and_class_values(data)
    #print docs, class_values
    
    search = cross_validate(docs, class_values)

    plane = search.best_estimator_.model.plane
    plot(data, plane, 'first_plane.pdf')

    # Remove negative class instances that the classifier gets right
    # i.e. true negatives 
    judgments = search.predict(docs)
    new_data = []
    new_docs = []
    new_class_values = []
    for i in range(len(data)):
        if not (class_values[i] == -1 and judgments[i] == -1):
            new_data.append(data[i])
            new_docs.append(docs[i])
            new_class_values.append(class_values[i])

    new_search = cross_validate(new_docs, new_class_values)
            

    for params, mean_score, scores in new_search.grid_scores_:
        print "%0.3f (+/-%0.03f) for %r" % (
            mean_score, scores.std() / 2, params)
   

    print new_search.best_score_, new_search.best_params_

    new_plane = new_search.best_estimator_.model.plane
    plot(new_data, new_plane, 'second_plane.pdf')
